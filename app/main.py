from fastapi import FastAPI, UploadFile, File
import camelot
import tempfile
import os
import pandas as pd

app = FastAPI()228


def normalize(text: str) -> str:
    if not isinstance(text, str):
        return ""
    return text.strip().lower()


def find_header_row(df: pd.DataFrame):
    """
    –ò—â–µ–º —Å—Ç—Ä–æ–∫—É, –≥–¥–µ –µ—Å—Ç—å —Å–ª–æ–≤–æ '–ü–∞—Ä–∞–º–µ—Ç—Ä'
    """
    for idx, row in df.iterrows():
        joined = " ".join(str(cell) for cell in row)
        if "–ø–∞—Ä–∞–º–µ—Ç—Ä" in joined.lower():
            return idx
    return None


def detect_columns(columns):
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–Ω–¥–µ–∫—Å—ã –∫–æ–ª–æ–Ω–æ–∫: –∏–º—è / –∑–Ω–∞—á–µ–Ω–∏–µ / –µ–¥–∏–Ω–∏—Ü–∞
    """
    name_col = value_col = unit_col = None

    for i, col in enumerate(columns):
        col_norm = normalize(col)

        if "–ø–∞—Ä–∞–º–µ—Ç—Ä" in col_norm:
            name_col = i
        elif "—Ä–µ–∑—É–ª—å—Ç" in col_norm:
            value_col = i
        elif "–µ–¥" in col_norm or "–∏–∑–º" in col_norm:
            unit_col = i

    return name_col, value_col, unit_col


def merge_multiline_rows(df: pd.DataFrame, name_col: int, value_col: int, unit_col: int):
    """
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–µ —è—á–µ–π–∫–∏ –≤ –æ–¥–Ω—É –∑–∞–ø–∏—Å—å
    """
    merged_rows = []
    i = 0
    
    while i < len(df):
        row = df.iloc[i]
        name = str(row.iloc[name_col]).strip()
        value = str(row.iloc[value_col]).strip() if value_col is not None else ""
        unit = str(row.iloc[unit_col]).strip() if unit_col is not None else ""
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ –∏–º–µ–Ω–∞
        if not name or name.lower() == "nan":
            i += 1
            continue
        
        # –ï—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ –ø—É—Å—Ç–æ–µ, —Å–º–æ—Ç—Ä–∏–º —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–æ–∫—É
        if not value or value.lower() == "nan":
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–æ–∫—É
            if i + 1 < len(df):
                next_row = df.iloc[i + 1]
                next_name = str(next_row.iloc[name_col]).strip()
                next_value = str(next_row.iloc[value_col]).strip() if value_col is not None else ""
                next_unit = str(next_row.iloc[unit_col]).strip() if unit_col is not None else ""
                
                # –ï—Å–ª–∏ —Å–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–æ–∫–∞ - –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ (–∏–º—è –ø—É—Å—Ç–æ–µ, –Ω–æ –µ—Å—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ)
                if (not next_name or next_name.lower() == "nan") and next_value and next_value.lower() != "nan":
                    value = next_value
                    if not unit or unit.lower() == "nan":
                        unit = next_unit
                    i += 2  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±–µ —Å—Ç—Ä–æ–∫–∏
                    merged_rows.append({"name": name, "value": value, "unit": unit})
                    continue
        
        # –û–±—ã—á–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ —Å –ø–æ–ª–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        if value and value.lower() != "nan":
            merged_rows.append({"name": name, "value": value, "unit": unit})
        
        i += 1
    
    return merged_rows


def extract_with_flavor(pdf_path: str, flavor: str):
    analytes = []

    tables = camelot.read_pdf(
        pdf_path,
        pages="all",
        flavor=flavor
    )

    print(f"\nüìÑ CAMELOT [{flavor.upper()}]: tables found = {len(tables)}")

    for table_index, table in enumerate(tables):
        df = table.df

        print(f"\n================ TABLE {table_index} RAW [{flavor}] =================")
        print(df.head(10))
        print("RAW COLUMNS:", df.columns.tolist())

        header_row = find_header_row(df)

        if header_row is None:
            print("‚ùå HEADER ROW WITH '–ü–∞—Ä–∞–º–µ—Ç—Ä' NOT FOUND")
            continue

        new_header = df.iloc[header_row]
        data_df = df.iloc[header_row + 1:].copy()
        data_df.columns = new_header
        
        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã iloc
        data_df = data_df.reset_index(drop=True)

        print(f"\n=========== TABLE {table_index} AFTER HEADER [{flavor}] =============")
        print(data_df.head(10))

        name_col, value_col, unit_col = detect_columns(data_df.columns)

        print("‚û°Ô∏è COLUMN MATCHING:")
        print(f"   name_col = {name_col}")
        print(f"   value_col = {value_col}")
        print(f"   unit_col = {unit_col}")

        if name_col is None or value_col is None:
            print("‚ùå REQUIRED COLUMNS NOT FOUND")
            continue

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–µ –∑–∞–ø–∏—Å–∏
        merged_data = merge_multiline_rows(data_df, name_col, value_col, unit_col)
        
        for item in merged_data:
            analytes.append({
                "raw_name": item["name"],
                "value": item["value"],
                "unit": item["unit"]
            })

        print(f"‚úÖ ANALYTES FOUND IN TABLE {table_index} [{flavor}]: {len(merged_data)}")

    print(f"\nüß™ TOTAL ANALYTES FOUND [{flavor.upper()}] = {len(analytes)}")
    return analytes


@app.post("/extract")
async def extract(file: UploadFile = File(...)):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(await file.read())
        pdf_path = tmp.name

    try:
        # 1Ô∏è‚É£ –ü—Ä–æ–±—É–µ–º lattice
        lattice_analytes = extract_with_flavor(pdf_path, flavor="lattice")

        if len(lattice_analytes) >= 10:
            print("üü¢ USING LATTICE RESULT")
            return {
                "count": len(lattice_analytes),
                "analytes": lattice_analytes,
                "method": "lattice"
            }

        # 2Ô∏è‚É£ fallback –Ω–∞ stream
        stream_analytes = extract_with_flavor(pdf_path, flavor="stream")

        print("üü° USING STREAM RESULT")
        return {
            "count": len(stream_analytes),
            "analytes": stream_analytes,
            "method": "stream"
        }

    finally:
        os.remove(pdf_path)